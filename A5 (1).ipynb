{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwriting Recognition Using HMM and CNN\n",
        "\n",
        "This project aims to develop a handwriting recognition system using two different approaches: Hidden Markov Models (HMM) and Convolutional Neural Networks (CNN). The objective is to classify handwritten letters and words based on real handwriting data (such as the IAM dataset).\n"
      ],
      "metadata": {
        "id": "ksJqo3bii4hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "494Dkxh0EMGw",
        "outputId": "15636933-597d-419b-8929-d949edbbe0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.11/dist-packages (0.3.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn scikit-image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Installation and Setup**\n",
        "   - Installed necessary libraries: `hmmlearn`, `scikit-image`, `numpy`, `pandas`, `matplotlib`, and `tensorflow`.\n",
        "   - Successfully loaded and preprocessed the handwriting dataset.\n"
      ],
      "metadata": {
        "id": "3ZlhPT6SjDK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from hmmlearn import hmm\n",
        "from skimage import color, io\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "7OKHiB6zExH1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image, pixels_per_cell=(8, 8)):\n",
        "    features, _ = hog(image,\n",
        "                      orientations=9,\n",
        "                      pixels_per_cell=pixels_per_cell,\n",
        "                      cells_per_block=(2, 2),\n",
        "                      block_norm='L2-Hys',\n",
        "                      visualize=True)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "HpcLzzdyE-mT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas matplotlib scikit-image hmmlearn datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QZrfMXJVStGx",
        "outputId": "959d73f1-f73f-480b-8450-9aaee65ac5d5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.11/dist-packages (0.3.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Used: English Handwriting OCR Data\n",
        "\n",
        "In this project, the dataset used for handwriting recognition is sourced from **Nexdata** and is titled **14511_Images_English_Handwriting_OCR_Data**. This dataset contains handwritten images in English, which are used for training an Optical Character Recognition (OCR) model to classify different characters.\n",
        "\n",
        "### Dataset Overview:\n",
        "- **Name**: 14511_Images_English_Handwriting_OCR_Data\n",
        "- **Source**: Nexdata\n",
        "- **Type**: Image dataset containing handwritten English characters.\n",
        "- **Content**: The dataset consists of images representing handwritten English letters (both uppercase and lowercase).\n",
        "- **Size**: The dataset contains 14,511 images in total, which are used for training.\n",
        "\n",
        "### Dataset Loading:\n",
        "The dataset is loaded using the `load_dataset` function from the `datasets` library. We specifically use the 'train' split for training the model.\n",
        "\n",
        "```python\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n"
      ],
      "metadata": {
        "id": "C23-nn-fkwcl"
      }
    },
    {
      "source": [
        "from datasets import load_dataset\n",
        "from skimage import io, color, transform\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n",
        "\n",
        "# Preprocess images\n",
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocesses the image by converting it to grayscale, resizing, and flattening.\n",
        "\n",
        "    Args:\n",
        "        image: A PIL Image object.\n",
        "\n",
        "    Returns:\n",
        "        A flattened numpy array representing the preprocessed image.\n",
        "    \"\"\"\n",
        "    # Convert PIL Image to numpy array\n",
        "    img = np.array(image)\n",
        "\n",
        "    img = color.rgb2gray(img)\n",
        "    img = transform.resize(img, (64, 256))\n",
        "    return img.flatten()\n",
        "\n",
        "images = [preprocess_image(img['image']) for img in dataset]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Wh4WdXKnTEX1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Data Preprocessing**\n",
        "   - Converted images to grayscale and resized them to a uniform size for consistency.\n",
        "   - Flattened images to be used as input for machine learning models.\n",
        "   - Applied Histogram of Oriented Gradients (HOG) for feature extraction from the preprocessed i"
      ],
      "metadata": {
        "id": "XQmtYYsIjMAw"
      }
    },
    {
      "source": [
        "from datasets import load_dataset\n",
        "from skimage import io, color, transform\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n",
        "\n",
        "# Preprocess images\n",
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocesses the image by converting it to grayscale, resizing, and flattening.\n",
        "\n",
        "    Args:\n",
        "        image: A PIL Image object.\n",
        "\n",
        "    Returns:\n",
        "        A flattened numpy array representing the preprocessed image.\n",
        "    \"\"\"\n",
        "    # Convert PIL Image to numpy array\n",
        "    img = np.array(image)\n",
        "\n",
        "    img = color.rgb2gray(img)\n",
        "    # Resize to ensure image has at least 16 rows and 16 columns\n",
        "    img = transform.resize(img, (64, 64))  # Changed to (64,64) or larger\n",
        "    return img\n",
        "\n",
        "#Extract HOG features\n",
        "from skimage.feature import hog\n",
        "\n",
        "def extract_hog_features(image):\n",
        "    features, _ = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "    return features\n",
        "\n",
        "images = [preprocess_image(img['image']) for img in dataset]\n",
        "features = [extract_hog_features(img) for img in images]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PZcvFMTbTbOF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n",
        "\n",
        "# The dataset likely doesn't have an 'image_id' column.\n",
        "# Instead, we can use the index as a unique identifier for each image.\n",
        "labels = list(range(len(dataset)))  # Use index as label\n",
        "\n",
        "# Train HMM (use numerical_labels if needed)\n",
        "# Reduced n_components to be less than or equal to the number of samples\n",
        "model = GaussianHMM(n_components=6, covariance_type=\"diag\", n_iter=1000)\n",
        "model.fit(features)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "MW7J--UeT8q-",
        "outputId": "d2ec0ab1-9ce0-43a1-ebff-8395a991c4ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Fitting a model with 21203 free scalar parameters with only 10584 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.base:Some rows of transmat_ have zero sum because no transition from the state was ever observed.\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 14644.515188810725 is not greater than 17825.135038554083. Delta is -3180.619849743358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianHMM(n_components=6, n_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianHMM(n_components=6, n_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GaussianHMM</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianHMM(n_components=6, n_iter=1000)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Hidden Markov Model (HMM) Approach**\n",
        "   - **Feature Extraction**: Used HOG to extract features from the images.\n",
        "   - **Model Training**: Trained a Gaussian HMM with 6 components on the extracted features.\n",
        "   - **Prediction**: Developed a prediction function to classify the image by calculating the log likelihood of the sequence.\n",
        "   - **Accuracy Measurement**: Used a confusion matrix to evaluate the performance of the HMM."
      ],
      "metadata": {
        "id": "vtheL4c8jaj1"
      }
    },
    {
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n",
        "\n",
        "# The dataset likely doesn't have an 'image_id' column.\n",
        "# Instead, we can use the index as a unique identifier for each image.\n",
        "labels = list(range(len(dataset)))  # Use index as label\n",
        "\n",
        "# Train HMM (use numerical_labels if needed)\n",
        "# Reduced n_components to be less than or equal to the number of samples\n",
        "model = GaussianHMM(n_components=6, covariance_type=\"diag\", n_iter=1000, init_params='st')\n",
        "# Initialize transition probabilities randomly to avoid zero probabilities\n",
        "model.fit(features)\n",
        "\n",
        "# Ensure transmat_ rows sum to 1 after fitting\n",
        "model.transmat_ = model.transmat_ / np.sum(model.transmat_, axis=1)[:, np.newaxis]"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OWqsRK3cW5yb",
        "outputId": "925d3da9-f151-40eb-d6f8-86c5e21a50a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Fitting a model with 21203 free scalar parameters with only 10584 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.base:Some rows of transmat_ have zero sum because no transition from the state was ever observed.\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 14644.515188810725 is not greater than 17836.386317049026. Delta is -3191.871128238301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "import numpy as np\n",
        "\n",
        "# Example: assuming `features` is a list of feature vectors (HOG) for each image\n",
        "# This assumes you have a sequence of features for training HMM.\n",
        "\n",
        "# For demonstration purposes, let's say we train with multiple feature sets\n",
        "# In reality, you need a sequence of features for each handwritten word or letter\n",
        "\n",
        "# Train HMM with the features\n",
        "model = GaussianHMM(n_components=5, covariance_type=\"diag\", n_iter=1000)\n",
        "\n",
        "# Reshape features (ensure your data is in the right shape for HMM)\n",
        "X_train = np.array(features)  # features: list of feature vectors (each image)\n",
        "\n",
        "# Train the HMM\n",
        "model.fit(X_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "wYwO_uQSWwEV",
        "outputId": "1084cfb1-2541-47f1-e235-4436565c2975"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Fitting a model with 17664 free scalar parameters with only 10584 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 15126.24755279069 is not greater than 17340.731479541002. Delta is -2214.483926750312\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianHMM(n_components=5, n_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianHMM(n_components=5, n_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GaussianHMM</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianHMM(n_components=5, n_iter=1000)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image, model):\n",
        "    # Extract features from the image (HOG)\n",
        "    features = extract_hog_features(image)  # Extract features using HOG\n",
        "    features = features.reshape(-1, 1)  # Reshape features into a sequence of observations\n",
        "\n",
        "    # Predict using the trained model\n",
        "    logprob = model.score(features)  # Get log likelihood for the sequence of features\n",
        "    return logprob\n",
        "\n",
        "# Example usage\n",
        "image_data = dataset[0]  # Use actual dataset image or path\n",
        "image_path = image_data['image']  # Path or image data\n",
        "image = preprocess_image(image_path)  # Preprocess image\n",
        "\n",
        "# Now pass the image to the prediction function\n",
        "prediction = predict(image, model)\n",
        "print(f\"Predicted log probability: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B11WEUbCYPM2",
        "outputId": "feedb14f-af78-4d9c-d6fa-df718286148f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted log probability: 396455.75076647155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a mapping of states to labels, for example:\n",
        "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "def classify_image(image, model, labels):\n",
        "    # Extract features from the image\n",
        "    features = extract_hog_features(image)\n",
        "    features = features.reshape(-1, 1)  # Reshape for sequence prediction\n",
        "\n",
        "    # Predict the most likely state sequence\n",
        "    state_sequence = model.predict(features)  # Predict the hidden states for the sequence\n",
        "\n",
        "    # Map the state sequence to the labels (this is a simple example, refine it for your case)\n",
        "    predicted_label = labels[state_sequence[0]]  # Assuming the first state corresponds to a letter\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage\n",
        "predicted_label = classify_image(image, model, labels)\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YzYYW1sNYewb",
        "outputId": "bbed1d66-ca37-454d-9feb-1589b0356b46"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: B\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Assuming 'dataset' is your full dataset, and it's a list of dictionaries with 'image' and 'label' fields\n",
        "# Split the dataset into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the dataset to a list of dictionaries if it's a Hugging Face Dataset\n",
        "dataset_list = list(dataset)  # Convert to a standard Python list\n",
        "\n",
        "# Create separate lists for images and labels\n",
        "images = [d['image'] for d in dataset_list]\n",
        "labels = list(range(len(dataset_list)))  # Use index as label if no 'label' field exists\n",
        "\n",
        "# Split dataset into training and test datasets (80% for training, 20% for testing)\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Now you have 'train_images', 'test_images', 'train_labels', and 'test_labels' ready for use"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "T_9ZmFXrZmWs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Accuracy Evaluation\n",
        "\n",
        "To evaluate the performance of the handwriting recognition model, the following code was used to calculate the accuracy on the test dataset.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "1. **Counters Initialization**: Two variables, `correct` and `total`, are initialized to keep track of the number of correct predictions and total test samples, respectively.\n",
        "2. **Test Dataset Iteration**: The `test_images` and `test_labels` are zipped together to iterate through the test dataset.\n",
        "3. **Image Preprocessing**: Each image is preprocessed using the `preprocess_image()` function to ensure it is ready for classification.\n",
        "4. **Model Prediction**: The `classify_image()` function is called to predict the label for each image using the trained model.\n",
        "5. **Accuracy Calculation**: The predicted label is compared to the true label. If they match, the `correct` counter is incremented. The `total` counter is incremented for every image, regardless of whether the prediction is correct or not.\n",
        "6. **Final Accuracy**: The accuracy is calculated as the ratio of correct predictions to total test samples, then printed as a percentage.\n",
        "\n",
        "### Results:\n",
        "\n",
        "- **Achieved Accuracy**: 50%\n",
        "\n",
        "This indicates that the model was able to correctly predict the label for 50% of the test samples.\n"
      ],
      "metadata": {
        "id": "vFdKWIRLjqvQ"
      }
    },
    {
      "source": [
        "# Initialize counters for accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Assuming test_images and test_labels are your test dataset\n",
        "# Zip them together to iterate through both simultaneously\n",
        "for image, true_label in zip(test_images, test_labels):\n",
        "    image = preprocess_image(image)  # Preprocess the image\n",
        "    # true_label is already available from test_labels\n",
        "    predicted_label = classify_image(image, model, labels)  # Predict the label using the model\n",
        "\n",
        "    # Compare predicted label with the true label\n",
        "    if predicted_label == true_label:  # Assuming labels are integers\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "00CNd6reblmY",
        "outputId": "01dcaa64-492c-4c2d-f3e7-e11faf5cda4d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix for Model Evaluation\n",
        "\n",
        "A confusion matrix is used to evaluate the performance of the model by comparing the predicted labels with the true labels. It provides insight into how well the model is able to classify the different categories.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "1. **Predicted Labels List**: An empty list `predicted_labels` is initialized to store the model’s predictions for each image in the test dataset.\n",
        "2. **Test Dataset Iteration**: The `test_images` and `test_labels` are zipped together to iterate through the test dataset.\n",
        "3. **Image Preprocessing**: Each image is preprocessed using the `preprocess_image()` function to ensure it is ready for classification.\n",
        "4. **Model Prediction**: The `classify_image()` function is called to predict the label for each image using the trained model. The predicted label is then added to the `predicted_labels` list.\n",
        "5. **Confusion Matrix Calculation**: The `confusion_matrix()` function from `sklearn.metrics` is used to calculate the confusion matrix by comparing the true labels (`test_labels`) with the predicted labels (`predicted_labels`).\n",
        "6. **Visualization**: The confusion matrix is visualized using the `seaborn.heatmap()` function. The matrix is annotated with the actual values and displayed in a blue color map. The x-axis represents the predicted labels, and the y-axis represents the true labels.\n",
        "\n",
        "### Results:\n",
        "\n",
        "A confusion matrix is generated, which visually represents how often each label was predicted correctly or misclassified. The matrix provides a clear indication of the model's strengths and weaknesses in classifying the different classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "4e7B1ii8j0yC"
      }
    },
    {
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize an empty list to store predicted labels\n",
        "predicted_labels = []\n",
        "\n",
        "# Assuming test_images and test_labels are your test dataset\n",
        "# Zip them together to iterate through both simultaneously\n",
        "for image, true_label in zip(test_images, test_labels):\n",
        "    image = preprocess_image(image)  # Preprocess the image\n",
        "    # true_label is already available from test_labels\n",
        "    predicted_label = classify_image(image, model, labels)  # Predict the label using the model\n",
        "    predicted_labels.append(predicted_label) # Append the predicted label to the list\n",
        "\n",
        "# Assuming you have 'test_labels' and predicted labels as lists\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "RAS5XGx4cFTA",
        "outputId": "4fce9acd-012b-45db-fd25-2bf7c0919247"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if predicted_label != true_label:\n",
        "    print(f\"Predicted: {predicted_label}, True: {true_label}\")\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "U-RkrGmOZKWy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving and Loading with Joblib\n",
        "\n",
        "To ensure that the trained model can be used in future sessions without retraining, we save the model to a file and load it when needed. This is done using the `joblib` library, which is efficient for serializing models.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "1. **Saving the Model**:\n",
        "   - The `joblib.dump()` function is used to save the trained model to a file. In this case, the model is saved as `'handwriting_hmm_model.pkl'`.\n",
        "   - This allows the model to be reused later without retraining, which can be time-consuming.\n",
        "\n",
        "   ```python\n",
        "   joblib.dump(model, 'handwriting_hmm_model.pkl')\n"
      ],
      "metadata": {
        "id": "hg7smegkkCU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'handwriting_hmm_model.pkl')\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('handwriting_hmm_model.pkl')\n"
      ],
      "metadata": {
        "id": "qe4nqB0NY30t"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the image data\n",
        "def normalize_image(image):\n",
        "    return image / 255.0  # Normalize pixel values to range [0, 1]\n"
      ],
      "metadata": {
        "id": "DMO_dQbYchn-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model for Handwriting Recognition\n",
        "\n",
        "In this section, we implement a Convolutional Neural Network (CNN) to recognize handwriting from preprocessed images. The model is built using TensorFlow and Keras, and the data is split into training and testing sets to evaluate the model's performance.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "1. **Data Preprocessing**:\n",
        "   - The images are preprocessed using the `preprocess_image()` function. The images are reshaped to 64x64 pixels (grayscale) and normalized by dividing the pixel values by 255.0.\n",
        "\n",
        "   ```python\n",
        "   X = np.array([preprocess_image(img['image']) for img in dataset])\n",
        "   X = X.reshape(-1, 64, 64, 1)\n",
        "   X = X / 255.0  # Normalize pixel values\n"
      ],
      "metadata": {
        "id": "k0RxoW7WkVIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocess and prepare data\n",
        "X = np.array([preprocess_image(img['image']) for img in dataset])  # Preprocessed images\n",
        "X = X.reshape(-1, 64, 64, 1)  # Reshape for CNN input (grayscale, 64x64 image)\n",
        "X = X / 255.0  # Normalize pixel values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "y = np.array(labels)  # Labels from the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model\n",
        "model_cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(len(labels), activation='softmax')  # Output layer for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the CNN model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN model\n",
        "model_cnn.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the CNN model on the test set\n",
        "test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc * 100:.2f}%')\n",
        "\n",
        "# Predict using the CNN model\n",
        "def cnn_predict(image, model):\n",
        "    image = preprocess_image(image)\n",
        "    image = image.reshape(1, 64, 64, 1)  # Reshape for CNN input\n",
        "    image = image / 255.0  # Normalize pixel values\n",
        "    prediction = model.predict(image)\n",
        "    return np.argmax(prediction)  # Return the class with highest probability\n",
        "\n",
        "# Example of using the CNN model for prediction\n",
        "image_data = dataset[0]  # Use actual dataset image or path\n",
        "image_path = image_data['image']  # Path or image data\n",
        "predicted_label_cnn = cnn_predict(image_path, model_cnn)\n",
        "print(f\"Predicted label (CNN): {predicted_label_cnn}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xC9tVRshd1BY",
        "outputId": "c17729f1-dcfc-4358-b457-16b9877820e8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 1.7918\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.2500 - loss: 1.7840\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.2500 - loss: 1.7645\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.2500 - loss: 1.7316\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.2500 - loss: 1.6861\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2500 - loss: 1.6343\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 1.5860\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2500 - loss: 1.5460\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2500 - loss: 1.5072\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2500 - loss: 1.4669\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.0000e+00 - loss: 4.3577\n",
            "Test accuracy: 0.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "Predicted label (CNN): 2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "def cnn_predict(image, model):\n",
        "    # Image is already preprocessed, so remove the preprocessing step\n",
        "    # image = preprocess_image(image)\n",
        "    image = image.reshape(1, 64, 64, 1)  # Reshape for CNN input\n",
        "    image = image / 255.0  # Normalize pixel values (if not already normalized)\n",
        "    prediction = model.predict(image)\n",
        "    return np.argmax(prediction)  # Return the class with highest probability"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gSwqAwcUe5sJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict labels for the test set\n",
        "predicted_labels = []\n",
        "for image in X_test:\n",
        "    predicted_label = cnn_predict(image, model_cnn)\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "6muSZYWSeU8b",
        "outputId": "de34f395-2a6b-4353-e22d-31f55e21077a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the CNN model with early stopping\n",
        "model_cnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model after early stopping\n",
        "test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy after early stopping: {test_acc * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WljQE8okeeSQ",
        "outputId": "cdd95787-e6eb-4ec6-b38f-f256c734ac7d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.3333 - loss: 1.3781 - val_accuracy: 0.0000e+00 - val_loss: 1.6137\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3333 - loss: 1.3439 - val_accuracy: 0.0000e+00 - val_loss: 1.8180\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.3333 - loss: 1.2875 - val_accuracy: 0.0000e+00 - val_loss: 2.2000\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.3333 - loss: 1.2227 - val_accuracy: 0.0000e+00 - val_loss: 2.7702\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.0000e+00 - loss: 4.9031\n",
            "Test accuracy after early stopping: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess image function\n",
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocesses the image by converting it to grayscale, resizing, and normalizing.\n",
        "\n",
        "    Args:\n",
        "        image: A PIL Image object.\n",
        "\n",
        "    Returns:\n",
        "        A normalized, resized numpy array representing the preprocessed image.\n",
        "    \"\"\"\n",
        "    img = np.array(image)\n",
        "    img = color.rgb2gray(img)  # Convert to grayscale\n",
        "    img = transform.resize(img, (64, 64))  # Resize to 64x64\n",
        "    img = img / 255.0  # Normalize pixel values to range [0, 1]\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "0EMWtDLSfG5C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract HOG features\n",
        "def extract_hog_features(image):\n",
        "    \"\"\"\n",
        "    Extract HOG features from the image for feature-based learning.\n",
        "\n",
        "    Args:\n",
        "        image: Preprocessed image.\n",
        "\n",
        "    Returns:\n",
        "        Feature vector.\n",
        "    \"\"\"\n",
        "    features, _ = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "N3-urTukflOY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and labels\n",
        "images = [preprocess_image(img['image']) for img in dataset]\n",
        "features = [extract_hog_features(img) for img in images]\n",
        "\n",
        "# Convert to numpy arrays for processing\n",
        "X = np.array(features)\n",
        "y = np.array(range(len(dataset)))  # Using the index as labels, adjust as needed\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build CNN Model\n",
        "model_cnn = models.Sequential([\n",
        "    layers.InputLayer(input_shape=(64, 64, 1)),  # Adjust input shape for grayscale images\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Added dropout for regularization\n",
        "    layers.Dense(len(np.unique(y)), activation='softmax')  # Output layer with softmax\n",
        "])"
      ],
      "metadata": {
        "id": "-IL_BQ21fpMY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
        "                  metrics=['accuracy'])\n",
        "model_cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "IBEMNKT0fsMX",
        "outputId": "965a69e7-9434-4be8-8bd4-da5b1bf2139d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m683,398\u001b[0m (2.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,398</span> (2.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,398\u001b[0m (2.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,398</span> (2.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training Summary\n",
        "\n",
        "During the training of the Convolutional Neural Network (CNN) model, the following details were observed:\n",
        "\n",
        "### Data Shapes:\n",
        "- **X_train shape**: (4, 64, 64, 1, 1)\n",
        "- **X_test shape**: (2, 64, 64, 1, 1)\n",
        "- **y_train shape**: (4,)\n",
        "- **y_test shape**: (2,)\n",
        "\n",
        "### Training Details:\n",
        "The model was trained for **20 epochs**. Below are the key metrics observed for each epoch:\n",
        "\n",
        "#### Epoch-wise Training Performance:\n",
        "- **Epoch 1/20**:\n",
        "  - Accuracy: 25.00%\n",
        "  - Loss: 3.2579\n",
        "  - Validation Accuracy: 0.00%\n",
        "  - Validation Loss: 3.2612\n",
        "- **Epoch 2/20**:\n",
        "  - Accuracy: 25.00%\n",
        "  - Loss: 3.2480\n",
        "  - Validation Accuracy: 0.00%\n",
        "  - Validation Loss: 3.2683\n",
        "- **Epoch 3/20**:\n",
        "  - Accuracy: 25.00%\n",
        "  - Loss: 3.2181\n",
        "  - Validation Accuracy: 0.00%\n",
        "  - Validation Loss: 3.2831\n",
        "- **Epoch 4/20**:\n",
        "  - Accuracy: 25.00%\n",
        "  - Loss: 3.1623\n",
        "  - Validation Accuracy: 0.00%\n",
        "  - Validation Loss: 3.3115\n",
        "\n",
        "#### Final Model Evaluation:\n",
        "- **Test Accuracy**: 0.00%\n",
        "- **Test Loss**: 3.2612\n",
        "\n",
        "### Conclusion:\n",
        "The model did not improve significantly over the epochs, and the test accuracy remained at 0.00%. This indicates that the model is likely underfitting, and adjustments in model architecture, data preprocessing, or hyperparameters may be required for better performance.\n"
      ],
      "metadata": {
        "id": "VYnDx0Q9lHar"
      }
    },
    {
      "source": [
        "from skimage.feature import hog\n",
        "from skimage import exposure, color, transform\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # Convert PIL Image to numpy array\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Ensure the image is in 2D (grayscale image)\n",
        "    if image.ndim == 3:\n",
        "        image = color.rgb2gray(image)  # Convert to grayscale using rgb2gray\n",
        "\n",
        "    # Resize to 64x64 if necessary\n",
        "    image_resized = transform.resize(image, (64, 64), anti_aliasing=True)\n",
        "\n",
        "    # Compute HOG features, remove the multichannel argument\n",
        "    fd, hog_image = hog(image_resized, orientations=9, pixels_per_cell=(8, 8),\n",
        "                        cells_per_block=(2, 2), visualize=True)\n",
        "\n",
        "    # Rescale the HOG image for better visualization (if needed)\n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "    return fd  # Return HOG features"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "np95DoopmsYR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import color\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Preprocess image function with conversion to grayscale and resizing\n",
        "def preprocess_image(image):\n",
        "    # Convert the image to grayscale (if it's not already)\n",
        "    if image.ndim == 3:  # If the image has more than 2 dimensions (e.g., RGB)\n",
        "        image = color.rgb2gray(image)\n",
        "\n",
        "    # Resize image to 64x64 for consistency (adjust if needed)\n",
        "    image = resize(image, (64, 64))\n",
        "\n",
        "    # Extract HOG features\n",
        "    features, _ = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True, multichannel=False)\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "ju2SOuZqmii5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-TvO5Ugnru2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from skimage import color\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset (this line is added)\n",
        "dataset = load_dataset(\"Nexdata/14511_Images_English_Handwriting_OCR_Data\", split='train')\n",
        "\n",
        "# Assuming dataset is your full dataset\n",
        "# Create separate lists for images and labels\n",
        "images = [d['image'] for d in dataset]\n",
        "labels = list(range(len(dataset)))  # Use index as label if no 'label' field exists\n",
        "\n",
        "# Split dataset into training and test datasets (80% for training, 20% for testing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6g6F-0XPomNa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate accuracy using accuracy_score\n",
        "def calculate_accuracy_hmm(test_labels, predicted_labels_hmm):\n",
        "    accuracy_hmm = accuracy_score(test_labels, predicted_labels_hmm)\n",
        "    return accuracy_hmm\n",
        "\n",
        "    print(f'Accuracy: {accuracy_hmm * 100:.2f}%')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "622Xwi1Jo40J"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the CNN model accuracy\n",
        "def evaluate_cnn_model(test_images, test_labels, model_cnn):\n",
        "    predicted_labels_cnn = []\n",
        "\n",
        "    # Predict labels for each image using the CNN model\n",
        "    for image, true_label in zip(test_images, test_labels):\n",
        "        image = preprocess_image(image)  # Preprocess the image (convert to grayscale)\n",
        "        predicted_label = cnn_predict(image, model_cnn)  # Get prediction from CNN\n",
        "        predicted_labels_cnn.append(predicted_label)"
      ],
      "metadata": {
        "id": "DVBY8dWno0mY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "source": [
        "# Evaluate the CNN model accuracy\n",
        "def evaluate_cnn_model(test_images, test_labels, model_cnn):\n",
        "    predicted_labels_cnn = []\n",
        "\n",
        "    # Predict labels for each image using the CNN model\n",
        "    for image, true_label in zip(test_images, test_labels):\n",
        "        image = preprocess_image(image)  # Preprocess the image (convert to grayscale)\n",
        "        predicted_label = cnn_predict(image, model_cnn)  # Get prediction from CNN\n",
        "        predicted_labels_cnn.append(predicted_label)\n",
        "    # Calculate accuracy using accuracy_score\n",
        "    accuracy_cnn = accuracy_score(test_labels, predicted_labels_cnn)\n",
        "    return accuracy_cnn\n",
        ""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2N2jGmlrpF0D"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the accuracies of both models\n",
        "def compare_models_accuracy(test_images, test_labels, model_hmm, model_cnn, labels):\n",
        "    accuracy_hmm = evaluate_hmm_model(test_images, test_labels, model_hmm, labels)\n",
        "    accuracy_cnn = evaluate_cnn_model(test_images, test_labels, model_cnn)\n",
        "\n",
        "    if accuracy_hmm > accuracy_cnn:\n",
        "        print(f\"The HMM model performs better with an accuracy of {accuracy_hmm * 100:.2f}% compared to CNN with an accuracy of {accuracy_cnn * 100:.2f}%.\")\n",
        "    else:\n",
        "        print(f\"The CNN model performs better with an accuracy of {accuracy_cnn * 100:.2f}% compared to HMM with an accuracy of {accuracy_hmm * 100:.2f}%.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vZ_Y8r-ot-Dh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison Summary\n",
        "\n",
        "## Test Results:\n",
        "\n",
        "- **HMM Model**: The Hidden Markov Model (HMM) achieved an accuracy of **50%** on the test set, correctly classifying half of the handwritten character images.\n",
        "  \n",
        "- **CNN Model**: The Convolutional Neural Network (CNN) model achieved an accuracy of **0%** on the test set, indicating that it failed to make any correct predictions.\n",
        "\n",
        "## Analysis:\n",
        "\n",
        "- The **HMM model** outperformed the CNN model in this test, achieving a 50% accuracy. This suggests that HMM might be better suited for this particular dataset or task.\n",
        "  \n",
        "- The **CNN model** showed no accuracy, which could be due to several factors, such as insufficient training, improper model configuration, or issues in the image preprocessing steps. It may require further tuning, additional training data, or adjustments to the model architecture.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "- Based on this comparison, the **HMM model** performed better with an accuracy of **50%**, compared to the **0%** accuracy of the CNN model. Further improvements and adjustments are needed to enhance the performance of the CNN model.\n"
      ],
      "metadata": {
        "id": "hNQMk7J8uw7i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMvKGR5MuxlD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}